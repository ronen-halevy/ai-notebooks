{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6889e01",
   "metadata": {},
   "source": [
    "# Optimizers Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46196cda",
   "metadata": {},
   "source": [
    "# Simple Demo 1D SGD with animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d288e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "\n",
    "def animate_optimizer(frame, opt, loss, lossf, x_sgd, x_cord_list, p2, p3):\n",
    "    step_count = opt.minimize(loss, var_list=[x_sgd]).numpy()\n",
    "    p3.set_data(x_sgd.numpy(), lossf(x_sgd.numpy()))\n",
    "    x_cord_list.append(x_sgd.numpy() )\n",
    "    loss_y = [lossf(xt) for xt in x_cord_list]\n",
    "    p2.set_data(x_cord_list, loss_y)\n",
    "    return p2, p3, x_sgd,\n",
    "\n",
    "\n",
    "\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=0.1)\n",
    "\n",
    "\n",
    "lossf = lambda x: (x ** 2)/2.0     \n",
    "\n",
    "x=list(np.linspace(-6, 6.0, 100))\n",
    "y = []\n",
    "for x_cord_list in x:\n",
    "    y.append(lossf(x_cord_list))\n",
    "\n",
    "x_cord_list = []\n",
    "\n",
    "plt.clf\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, y)\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_xlabel('w1')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('Gradient Descent')\n",
    "\n",
    "## 1d 1single weight loss plot\n",
    "\n",
    "x_sgd = tf.Variable(5.0)\n",
    "p1, = ax.plot([x_sgd], [lossf(x_sgd)], 'k')\n",
    "p2, = ax.plot([], [], color='red', alpha=0.6)\n",
    "p2, = ax.plot([], [],  'r.', alpha=.5)\n",
    "\n",
    "p3, = ax.plot([], [],  'r.', alpha=.5, ms=10)\n",
    "max_iter = 100\n",
    "loss = lambda: (x_sgd ** 2)/2.0      \n",
    "\n",
    "\n",
    "anim2 = animation.FuncAnimation(fig, animate_optimizer, \n",
    "                                frames=range(0, max_iter),  fargs = (opt, loss, lossf, x_sgd, x_cord_list, p2, p3), interval=50,repeat=True, repeat_delay=20)\n",
    "filename = 'sgd_1d_intro.gif'\n",
    "\n",
    "writergif = animation.PillowWriter(fps=5) \n",
    "anim2.save('output/'+filename, writer=writergif)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbb539b",
   "metadata": {},
   "source": [
    "# Optimizers Graph Generation - Loops on optimizers list, and generates 3d animations with various elevations, and 2D contour animation per each optimizer. All outputs are saved to gif files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2829a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizers Graph Generation - Loops on aoptimizers list, and generate 3d animations with various elevations and 2D contour animation per each optimizer\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "import mpl_toolkits.mplot3d.axes3d as p3\n",
    "# from mpl_toolkits.mplot3d import axes3d, Axes3D #<-- Note the capitalization! \n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "\n",
    "####### #3D anaimate and contour plot\n",
    "\n",
    "def animate_optimizer_3d(frame, opt, loss_func, loss_func_args, x_cord, y_cord, x_cord_list, yy, zz, ax, p2, p3):  \n",
    "    step_count = opt.minimize(loss_func, var_list=[y_cord, x_cord]).numpy()\n",
    "    ax.title.set_text('3D Plot, Frame={}'.format(frame))\n",
    "    p3.set_data(x_cord.numpy(), y_cord.numpy())\n",
    "    p3.set_3d_properties(loss_func_args(x_cord.numpy(), y_cord.numpy())) \n",
    "    p2.set_data(np.array(x_cord_list), np.array(yy))\n",
    "    p2.set_3d_properties(np.array(zz)) \n",
    "    x_cord_list.append(x_cord.numpy())\n",
    "    yy.append(y_cord.numpy())\n",
    "    zz.append(loss_func_args(x_cord.numpy(), y_cord.numpy())) \n",
    "    return p3, x_cord, y_cord\n",
    "\n",
    "\n",
    "def plot_loss_func_3d(loss_func_args):\n",
    "    x_cord_list = np.linspace(-5.5, 5.5, 100)\n",
    "    yy = np.linspace(-5.5, 5.5, 100)\n",
    "    X, Y = np.meshgrid(x_cord_list, yy)\n",
    "    Z = loss_func_args(X, Y)\n",
    "    fig1 = plt.figure(figsize=(16, 6))\n",
    "    fig1.set_figheight(10)\n",
    "    fig1.set_figwidth(10)\n",
    "    ax1 = plt.axes(projection='3d')\n",
    "    surf = ax1.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.3, \n",
    "                           linewidth=0, antialiased=False)\n",
    "    ax1.set_xlabel('w1', fontsize=20)\n",
    "    ax1.set_ylabel('w2', fontsize=20)\n",
    "    ax1.set_zlabel('J(w1, w2)', fontsize=20)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.yaxis.set_ticklabels([])\n",
    "    ax1.zaxis.set_ticklabels([])\n",
    "    return fig1, ax1\n",
    "\n",
    "\n",
    "####### #3D anaimate and contour plot\n",
    "\n",
    "def animate_contour_2d(frame, opt, loss_func, \n",
    "                       loss_func_args, \n",
    "                       x_cord, y_cord, x_cord_list, yy, ax, \n",
    "                       p2, p3):   \n",
    "\n",
    "    step_count = opt.minimize(loss_func, var_list=[y_cord, x_cord]).numpy()\n",
    "    ax.title.set_text('2D Contour Plot, Frame={}'.format(frame))\n",
    "    if step_count > -10:\n",
    "        p3.set_data([x_cord.numpy()], [y_cord.numpy()])\n",
    "\n",
    "        p2.set_data(np.array(x_cord_list), np.array(yy))\n",
    "\n",
    "        x_cord_list.append(x_cord.numpy())\n",
    "        yy.append(y_cord.numpy())\n",
    "\n",
    "    return p3, x_cord, y_cord\n",
    "\n",
    "def plot_contour(loss_with_args):\n",
    "    x = np.linspace(-5.5, 5.5, 100)\n",
    "    y = np.linspace(-5.5, 5.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100, 121, 144,169, 196, 225, 256, 289, 324, 361, 400, 441, 484, 529]\n",
    "    Z = loss_with_args(X,Y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contour(X, Y, Z, levels, colors='black')\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "# Configurations\n",
    "alpha=learning_rate = 0.1 \n",
    "\n",
    "\n",
    "# Set loss func\n",
    "y_coeff_moderate = 15\n",
    "y_coeff = 20\n",
    "x_coeff = 1\n",
    "loss_func_args_symetric = lambda x_cord, y_cord: (x_cord ** 2)/2.0  + (y_cord ** 2)/2.0 \n",
    "loss_func_symetric = lambda: (x_cord ** 2)/2.0  + (y_cord ** 2)/2.0\n",
    "\n",
    "loss_func_args_asymetric_stable  = lambda x_cord, y_cord: x_coeff * (x_cord ** 2)/2.0  + y_coeff_moderate*(y_cord ** 2)/2.0 \n",
    "loss_func_asymetric_stable = lambda: x_coeff * (x_cord ** 2)/2.0  + y_coeff_moderate*(y_cord ** 2)/2.0\n",
    "\n",
    "loss_func_args_y_steep = lambda x_cord, y_cord: x_coeff * (x_cord ** 2)/2.0  + y_coeff*(y_cord ** 2)/2.0 \n",
    "loss_func_y_steep = lambda: x_coeff * (x_cord ** 2)/2.0  + y_coeff*(y_cord ** 2)/2.0\n",
    "\n",
    "# Set desired iteration: filenames and elevetion, e.g. provide various eleveation views\n",
    "algorythms_list = [\n",
    "\n",
    "    {\n",
    "        'name': 'sgd',\n",
    "        'num_of_iter': 55,\n",
    "        'opt': tf.keras.optimizers.SGD(learning_rate=alpha), \n",
    "        'plot_3d': True,\n",
    "    },\n",
    "    {\n",
    "        'name': 'momentum_sgd',\n",
    "        'num_of_iter': 100,\n",
    "        'opt': tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9, nesterov=False), \n",
    "        'plot_3d': True\n",
    "     },\n",
    "\n",
    "    {\n",
    "        'name': 'nesterov_sgd',\n",
    "        'num_of_iter': 60,\n",
    "        'opt': tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9, nesterov=True), \n",
    "        'plot_3d': True\n",
    "    },\n",
    "       {\n",
    "        'name': 'adagrad',\n",
    "        'opt': tf.keras.optimizers.Adagrad(learning_rate=alpha,initial_accumulator_value=0.1,epsilon=1e-07,name=\"Adagrad\"),\n",
    "        'num_of_iter': 180,\n",
    "        'plot_3d': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'rmsprop',\n",
    "        'opt': tf.keras.optimizers.RMSprop(learning_rate=alpha, rho=0.9, momentum=0.0, epsilon=1e-07, centered=False, name=\"RMSprop\"),\n",
    "        'num_of_iter': 80,\n",
    "        'plot_3d': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'adadelta',\n",
    "        'opt': tf.keras.optimizers.Adadelta(learning_rate=alpha, rho=0.95, epsilon=1e-07, name=\"Adadelta\"),\n",
    "        'num_of_iter': 80,\n",
    "        'plot_3d': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'adam',\n",
    "        'num_of_iter': 120,\n",
    "        'opt': tf.keras.optimizers.Adam(learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,name='Adam'),\n",
    "        'plot_3d': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'adamax',\n",
    "        'num_of_iter': 150,\n",
    "        'opt': tf.keras.optimizers.Adamax(learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07,name='Adamax'), \n",
    "        'plot_3d': True\n",
    "    },\n",
    "    {\n",
    "        'name': 'nadam',\n",
    "        'opt': tf.keras.optimizers.Nadam(learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, name=\"Nadam\"),\n",
    "        'num_of_iter': 120,\n",
    "        'plot_3d': True\n",
    "    },\n",
    "\n",
    "     ]\n",
    "\n",
    "loss_funcs_list =  [ {\n",
    "        'name': 'converge',\n",
    "        'loss_func_args': loss_func_args_symetric, \n",
    "        'loss_func': loss_func_symetric\n",
    "    },\n",
    "    {\n",
    "        'name': 'stable',\n",
    "        'loss_func_args': loss_func_args_asymetric_stable, \n",
    "        'loss_func':loss_func_asymetric_stable\n",
    "    }, \n",
    "\n",
    "    {\n",
    "        'name': 'steep',\n",
    "        'loss_func_args': loss_func_args_y_steep , \n",
    "        'loss_func': loss_func_y_steep\n",
    "    }\n",
    " ]\n",
    "       \n",
    "camera_positions = [{'elev': 0, 'azim': 0}, {'elev': 0, 'azim': 30}, {'elev': 90, 'azim': 30}]\n",
    "\n",
    "default_num_of_iter = 20\n",
    "# Main loop, runs on various algorithms\n",
    "for algorithm in algorythms_list:\n",
    "    max_iter = algorithm.get('num_of_iter', default_num_of_iter)\n",
    "    opt = algorithm['opt']\n",
    "    alg_name = algorithm['name']\n",
    "    # loop on loss functions\n",
    "    for loss_funcs in loss_funcs_list: \n",
    "        test_name = loss_funcs['name']\n",
    "\n",
    "        loss_func_args = loss_funcs['loss_func_args']\n",
    "        loss_func = loss_funcs['loss_func']\n",
    "\n",
    "        ## Now plot 2d contour\n",
    "        x_cord = tf.Variable(5.0)\n",
    "        y_cord = tf.Variable(5.0)\n",
    "        x_cord_list = []\n",
    "        yy = []\n",
    "        filename = '2d_contour_' + alg_name + '_' + test_name +'.gif'\n",
    "\n",
    "        fig_contour, ax_contour = plot_contour(loss_func_args)\n",
    "\n",
    "        ax_contour.plot(x_cord, x_cord, \"x-\", color='red', alpha=0.6)\n",
    "        p2_contour, = ax_contour.plot([], [], \".-\", color='red', alpha=0.6)\n",
    "        p3_contour, = ax_contour.plot([], [], \"o-\", color='blue', alpha=0.6)\n",
    "        ax_contour.set_xlabel('w1')\n",
    "        ax_contour.set_ylabel('w2')\n",
    "        ax_contour.set_yticklabels([])\n",
    "        ax_contour.set_xticklabels([])\n",
    "        ax_contour.set_title('Optimization using {name} - Contour Plot'.format(name=filename))\n",
    "        anim2 = animation.FuncAnimation(fig_contour, animate_contour_2d, frames=range(0, max_iter), fargs=(opt, loss_func, loss_func_args, x_cord, y_cord, x_cord_list, yy, ax_contour, p2_contour, p3_contour), interval=30,repeat=True, repeat_delay=20)\n",
    "        writergif = animation.PillowWriter(fps=5) \n",
    "        anim2.save('output/'+filename, writer=writergif)\n",
    "\n",
    "\n",
    "        # Internal loop, relevant to 3D contour: camera positions\n",
    "        \n",
    "        if not algorithm.get('plot_3d'):\n",
    "            continue\n",
    "            \n",
    "        for camera_position in camera_positions:\n",
    "           \n",
    "            x_cord = tf.Variable(5.0)\n",
    "            y_cord = tf.Variable(5.0)\n",
    "            x_cord_list = []\n",
    "            yy = []\n",
    "            zz = []\n",
    "\n",
    "            azim = camera_position['azim']\n",
    "            elev = camera_position['elev']\n",
    "            plt.clf\n",
    "            # plot loss func as a background:\n",
    "      \n",
    "            fig, ax = plot_loss_func_3d(loss_func_args)\n",
    "\n",
    "            title = ax.set_title('3D View')\n",
    "\n",
    "            z = loss_func_args(x_cord.numpy(), y_cord.numpy())\n",
    "            p3, = ax.plot(x_cord.numpy(), y_cord.numpy(), z,  'bo')\n",
    "            p2, = ax.plot(np.array([x_cord.numpy()]), np.array([y_cord.numpy()]), np.array([z]), 'r.-')\n",
    "            if camera_position:\n",
    "                ax.view_init(azim=camera_position['azim'], elev=camera_position['elev'])\n",
    "            filename = '3d_contour_'  + alg_name + '_' + test_name\n",
    "            if elev not in [None, 'Default']:\n",
    "                filename = filename + '_azim_' + str(azim) +  '_elev_' + str(elev)\n",
    "            filename = filename + '.gif'\n",
    "\n",
    "            anim1 = animation.FuncAnimation(fig, animate_optimizer_3d, frames=range(0, max_iter),\n",
    "                                            fargs=(opt, loss_func, loss_func_args, x_cord, y_cord, x_cord_list, yy, zz, ax, p2, p3), blit=True, interval=30,repeat=True, repeat_delay=20)\n",
    "            writergif = animation.PillowWriter(fps=5) \n",
    "            anim1.save('output/'+filename, writer=writergif)\n",
    "\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2651054",
   "metadata": {},
   "source": [
    "# SGD and momentum together - no animation #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27e610e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradeint Descent with large step size with animation\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import matplotlib.animation as animation\n",
    "\n",
    "# %matplotlib widget\n",
    "# %matplotlib notebook\n",
    "\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "\n",
    "X_COEF=1\n",
    "Y_COEF=20\n",
    "alpha = 0.1 # 0.02517\n",
    "x_offset = 2\n",
    "y_offset = 1.5\n",
    "\n",
    "\n",
    "loss_with_args = lambda x_cord, y_cord: X_COEF*(x_cord)**2 + Y_COEF*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "loss = lambda: X_COEF*(x_cord)**2 + Y_COEF*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "loss_with_args_1d = lambda  y_cord: X_COEF*(x_cord)**2 + Y_COEF*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "\n",
    "\n",
    "def plot_contour_2d():\n",
    "    x = np.linspace(-5.5, 5.5, 100)\n",
    "    y = np.linspace(-5.5, 5.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100, 121, 144,169, 196, 225, 256, 289]\n",
    "    Z = loss_with_args(X,Y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contour(X, Y, Z, levels, colors='black')\n",
    "    return fig,ax\n",
    "\n",
    "\n",
    "def plot_contour_1d():\n",
    "    x = np.linspace(-5.5, 5.5, 100)\n",
    "    y = np.linspace(-5.5, 5.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100, 121, 144,169, 196, 225, 256, 289]\n",
    "    Z = loss_with_args(X,Y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(x, Z, levels, colors='black')\n",
    "    return fig,ax\n",
    "\n",
    "def do_optimize(opt, num_iterations, name):\n",
    "    loss = lambda: X_COEF*(x_cord)**2 + Y_COEF*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "\n",
    "    fig, ax = plot_contour_2d()\n",
    "    x_cord = tf.Variable(-5.0)\n",
    "    y_cord = tf.Variable(-2.0)\n",
    "    x_cord_list = []\n",
    "    yy = []\n",
    "    for j in range(num_iterations):\n",
    "        step_count = opt.minimize(loss, var_list=[x_cord, y_cord]).numpy()\n",
    "        x_cord_list.append(x_cord.numpy())\n",
    "        yy.append(y_cord.numpy())\n",
    "\n",
    "    ax.plot(x_cord_list, yy, \"o-\")\n",
    "    ax.set_xlabel('Bias')\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_title('Optimization using {name} - Contour Plot'.format(name=name))\n",
    "\n",
    "\n",
    "# SGD\n",
    "num_iterations = 100\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=alpha)\n",
    "do_optimize(opt, num_iterations, name = \"SGD\")\n",
    "\n",
    "# Momentum\n",
    "opt = tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9)\n",
    "do_optimize(opt, num_iterations, name = \"Momentum\")\n",
    "\n",
    "# ADADELTA\n",
    "num_iterations = 100\n",
    "\n",
    "opt = tf.keras.optimizers.Adadelta(learning_rate=5, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
    "opt = keras.optimizers.Adadelta(learning_rate=0.001, rho=0.95, epsilon=1e-07, name=\"Adadelta\")\n",
    "do_optimize(opt, num_iterations, name = \"ADADELTA\")\n",
    "\n",
    "\n",
    "# ADAM\n",
    "num_iterations = 80\n",
    "\n",
    "opt = tf.keras.optimizers.Adam(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'\n",
    ")\n",
    "\n",
    "opt = do_optimize(opt, num_iterations, name = \"Adam\")\n",
    "\n",
    "\n",
    "# ADAMAX\n",
    "\n",
    "opt = tf.keras.optimizers.Adamax(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax'\n",
    ")\n",
    "\n",
    "\n",
    "do_optimize(opt, num_iterations, name = \"Adamax\")\n",
    "\n",
    "\n",
    "# var1 = tf.Variable(10.0)\n",
    "# loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1\n",
    "# step_count = opt.minimize(loss, [var1]).numpy()\n",
    "# The first step is `-learning_rate*sign(grad)`\n",
    "# var1.numpy()\n",
    "\n",
    "# opimize(opt, ax)\n",
    "\n",
    "# Adagrad\n",
    "num_iterations=600\n",
    "opt = tf.keras.optimizers.Adagrad(\n",
    "    learning_rate=alpha, initial_accumulator_value=0.1, epsilon=1e-07,\n",
    "    name='Adagrad'\n",
    ")\n",
    "do_optimize(opt, num_iterations, name = \"Adagrad\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17872dd8",
   "metadata": {},
   "source": [
    "# Optimizers with contours - Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e5b12aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "\n",
    "\n",
    "X_COEF=1\n",
    "Y_COEF=40\n",
    "alpha = 0.02517\n",
    "x_offset = 2\n",
    "y_offset = 1.5\n",
    "\n",
    "\n",
    "\n",
    "def plot_contour(loss_with_args, title):\n",
    "\n",
    "    x = np.linspace(-5.5, 5.5, 100)\n",
    "    y = np.linspace(-5.5, 5.5, 100)\n",
    "    X, Y = np.meshgrid(x, y)\n",
    "    levels = [0.1,1,2,4,9, 16, 25, 36, 49, 64, 81, 100, 121, 144,169, 196, 225, 256, 289]\n",
    "    Z = loss_with_args(X,Y)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.contour(X, Y, Z, levels, colors='black')\n",
    "    ax.set_yticklabels([])\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_xlabel('w1')\n",
    "    ax.set_ylabel('b')\n",
    "\n",
    "    ax.set_title(title)\n",
    "    return fig,ax\n",
    "\n",
    "def init_animate_optimizer():\n",
    "    global xs\n",
    "    global x_cord\n",
    "    global y_cord\n",
    "    global opt\n",
    "    global x_cord_list\n",
    "    global yy\n",
    "    \n",
    "\n",
    "    x_cord = tf.Variable(-5.0)\n",
    "    y_cord = tf.Variable(-2.0)\n",
    "    x_cord_list = []\n",
    "    yy = []\n",
    "    return p1,\n",
    "\n",
    "\n",
    "\n",
    "x_cord_list = []\n",
    "yy = []\n",
    "def animate_optimizer(frame, opt, loss_func):\n",
    "    global x\n",
    "    global x_cord\n",
    "    global y_cord\n",
    "    global x_cord_list\n",
    "    global yy\n",
    "    global p2, p3\n",
    "   \n",
    "    step_count = opt.minimize(loss_func, var_list=[x_cord, y_cord]).numpy()\n",
    "\n",
    "    p2.set_data(x_cord_list, yy)\n",
    "    p3.set_data(x_cord.numpy(), y_cord.numpy())\n",
    "    x_cord_list.append(x_cord.numpy())\n",
    "    yy.append(y_cord.numpy())\n",
    "    return p2,\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ed6409",
   "metadata": {},
   "source": [
    "# Run optimizations - static plots with loss_diff_in_gradients - first execute  box above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import animation, rc\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "\n",
    "def do_optimize(loss_with_args, opt, num_iterations, name):\n",
    "\n",
    "\n",
    "    fig, ax = plot_contour(loss_with_args, name)\n",
    "\n",
    "    x_cord_list = []\n",
    "    yy = []\n",
    "    \n",
    "    x_cord = tf.Variable(-5.0)\n",
    "    y_cord = tf.Variable(-2.0)\n",
    "    loss_func = lambda: 1*(x_cord)**2 + 40*(y_cord)**2  \n",
    "\n",
    "\n",
    "\n",
    "    for j in range(num_iterations):\n",
    "        step_count = opt.minimize(loss_func, var_list=[x_cord, y_cord])\n",
    "        x_cord_list.append(x_cord.numpy())\n",
    "        yy.append(y_cord.numpy())\n",
    "\n",
    "    ax.plot(x_cord_list, yy, \"o-\")\n",
    "    ax.set_xlabel('Bias')\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_title('Optimization using {name} - Contour Plot'.format(name=name))\n",
    "      \n",
    "    ####  Anther loss func ###############################\n",
    "    fig, ax = plot_contour(lambda x, y:  (x)**4 - 10 * (x)** 2 - 3 * (x)+ 40*(y)**2 , name)\n",
    "\n",
    "    x_cord = tf.Variable(-5.0)\n",
    "    y_cord = tf.Variable(-2.0)\n",
    "    x_cord_list = []\n",
    "    yy = []\n",
    "    loss_func = lambda:  x_cord**4 - 10 * x_cord** 2 - 3 * x_cord + 40*y_cord**2 \n",
    "    \n",
    "    for j in range(num_iterations):\n",
    "        step_count = opt.minimize(loss_func, var_list=[x_cord, y_cord])\n",
    "        x_cord_list.append(x_cord.numpy())\n",
    "        yy.append(y_cord.numpy())\n",
    "\n",
    "    ax.plot(x_cord_list, yy, \"o-\")\n",
    "    ax.set_xlabel('Bias')\n",
    "    ax.set_ylabel('Weight')\n",
    "    ax.set_title('Optimization using {name} - Contour Plot'.format(name=name))\n",
    "    \n",
    "#     fig1, ax1 = plt.subplots()\n",
    "#     xcoords = np.linspace(-3,3,100)\n",
    "#     ycoord = [loss_func_y(xcoord)  for xcoord in xcoords]\n",
    "#     ax1.plot(xcoords, ycoord)\n",
    "    \n",
    "#     loss_coord = [loss_func_y(y_coord)  for y_coord in yy]\n",
    "#     ax1.plot(yy, loss_coord, 'o-', color='red', alpha=0.6)\n",
    "#     ax1.set_xlabel('Weight')\n",
    "#     ax1.set_ylabel('Loss')\n",
    "#     ax1.set_title('Weight Loss using {name} -  Plot'.format(name=name))                  \n",
    "\n",
    "\n",
    "def run_optimization_static_contour_graph(loss_func_with_args):\n",
    "#     x=loss_func().numpy()\n",
    "#     print(x)\n",
    "#     alpha = 0.0067\n",
    "    optimizers_list = []\n",
    "    optimizers_list.append({'name': 'sgd', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha), 'max_iter': 100})\n",
    "    optimizers_list.append({'name': 'momentum', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9), 'max_iter': 100})\n",
    "    optimizers_list.append({'name': 'adam', 'opt':  tf.keras.optimizers.Adam(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), 'max_iter': 450})\n",
    "    optimizers_list.append({'name': 'adamax', 'opt':  tf.keras.optimizers.Adamax(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax'), 'max_iter': 450})\n",
    "    \n",
    "    \n",
    "    for optimizer in optimizers_list:\n",
    "        print(optimizer['name'])\n",
    "        opt = optimizer['opt']\n",
    "        plt.clf\n",
    "#         fig1, ax = plot_contour(loss_with_args=loss_func_with_args, title=optimizer['name'])\n",
    "        p1, = ax.plot([tf.Variable(-5.0)], [tf.Variable(-2.0)], 'kx')\n",
    "        p2, = ax.plot([], [], color='red', alpha=0.6)\n",
    "        p3, = ax.plot([], [], 'r.')\n",
    "        max_iter = optimizer['max_iter']\n",
    "        do_optimize(loss_with_args=loss_func_with_args, opt=opt, num_iterations=max_iter, name=optimizer['name'])\n",
    "\n",
    "\n",
    "x_cord = tf.Variable(-5.0)\n",
    "y_cord = tf.Variable(-2.0)\n",
    "loss_func = lambda: 1*(x_cord)**2 + 40*(y_cord)**2  \n",
    "\n",
    "# loss_func = lambda: 1*(x_cord)**2 + 40*(y_cord)**2  \n",
    "loss_func_with_args = lambda x_cord, y_cord: 1*(x_cord)**2 + 40*(y_cord)**2        \n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "run_optimization_static_contour_graph(loss_func_with_args=loss_func_with_args)\n",
    "# out_file_name_prefix = 'loss_diff_gradients_2d_contour_'\n",
    "                                                      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995e2587",
   "metadata": {},
   "source": [
    "# Scratch static contour with saddle point - fix it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f82dc9b",
   "metadata": {},
   "source": [
    "# scratch 2:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75dfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # #####3D:\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib import cm\n",
    "import matplotlib.animation as animation\n",
    "from matplotlib import animation, rc\n",
    "\n",
    "%matplotlib widget\n",
    "%matplotlib notebook\n",
    "\n",
    "max_iter=100\n",
    "\n",
    "X_COEF=1 #20\n",
    "Y_COEF=40\n",
    "alpha = 0.02517\n",
    "\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "max_iter = 70\n",
    "\n",
    "alpha = 0.0067\n",
    "x_offset = 0\n",
    "y_offset = 0\n",
    "max_iter= 150\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def f2(x):\n",
    "    return X_COEF*(x[0]+x_offset)**2 + Y_COEF*(x[1]+y_offset)**2\n",
    "\n",
    "def grad2(x):\n",
    "    return np.array([4 * x[0]**3 - 20 * x[0] - 3, 2*Y_COEF*(x[1]+y_offset)])\n",
    "    #return np.array([4 * x**3 - 10 * x - 3, 2*Y_COEF*(x[1]+y_offset)])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "v = 0\n",
    "\n",
    "def gd2_momentum_1(x, frame, alpha, grad=grad2, beta=0.9):\n",
    "    global v\n",
    "    v = beta*v + (1-beta)*grad(x)\n",
    "    vc = v/(1+beta**(frame+1))\n",
    "    x = x - alpha * vc\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "rc('animation', html='html5')\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "\n",
    "def cost_func(x, y):\n",
    "    Z = (x +x_offset)**4 - 20 * (x +x_offset)** 2 - 3 * (x +x_offset)+ Y_COEF*(y+y_offset)**2\n",
    "#     Z = X_COEF*(X+x_offset)**2 + Y_COEF*(Y+y_offset)**2\n",
    "    return Z\n",
    "\n",
    "def plot_loss_func_3d():\n",
    "    x_cord_list = np.linspace(-5.5-x_offset, 5.5-y_offset, 50)\n",
    "    yy = np.linspace(-5.5-x_offset, 5.5-y_offset, 50)\n",
    "    X, Y = np.meshgrid(x_cord_list, yy)\n",
    "    Z = cost_func(X, Y)\n",
    "    fig1 = plt.figure(figsize=(16, 6))\n",
    "    ax1 = fig1.gca(projection='3d')\n",
    "\n",
    "    surf = ax1.plot_surface(X, Y, Z, rstride=1, cstride=1, alpha=0.5, \n",
    "                           linewidth=0, antialiased=False)\n",
    "\n",
    "    ax1.set_xlabel('b', fontsize=20)\n",
    "    ax1.set_ylabel('w1', fontsize=20)\n",
    "    ax1.set_zlabel('J(b, w)', fontsize=20)\n",
    "    ax1.xaxis.set_ticklabels([])\n",
    "    ax1.yaxis.set_ticklabels([])\n",
    "    ax1.zaxis.set_ticklabels([])\n",
    "    plt.show()\n",
    "    return fig1, ax1\n",
    "\n",
    "# plot_loss_func_3d()\n",
    "x = np.linspace(0,5,100)\n",
    "loss = lambda x: 2*x **4 - 20 * x** 2# - 3 * x\n",
    "loss = lambda x: 4.0*tf.cos(x-1)+tf.divide(tf.cos(2.0*np.pi*x),x)\n",
    "y = loss(x)\n",
    "plt.plot(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8467d823",
   "metadata": {},
   "source": [
    "# Run optimizations - Contour Animation with loss_diff_in_gradients - first execute  box above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1ea208",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_diff_in_gradients = lambda: 1*(x_cord)**2 + 40*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "loss_func = loss_diff_in_gradients\n",
    "loss_with_args = lambda x_cord, y_cord: X_COEF*(x_cord)**2 + Y_COEF*(y_cord)**2         # d(loss)/d(var1) = var1\n",
    "out_file_name_prefix = 'loss_diff_gradients_2d_contour_'\n",
    "\n",
    "optimizers_list = []\n",
    "\n",
    "optimizers_list.append({'name': 'sgd', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha), 'max_iter': 100})\n",
    "optimizers_list.append({'name': 'momentum', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9), 'max_iter': 100})\n",
    "optimizers_list.append({'name': 'adam', 'opt':  tf.keras.optimizers.Adam(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), 'max_iter': 200})\n",
    "optimizers_list.append({'name': 'adam', 'opt':  tf.keras.optimizers.Adamax(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax'), 'max_iter': 200})\n",
    "\n",
    "\n",
    "                   \n",
    "for optimizer in optimizers_list:\n",
    "    print(optimizer['name'])\n",
    "    opt = optimizer['opt']\n",
    "    plt.clf\n",
    "    fig1, ax = plot_contour(loss_with_args=loss_with_args, title=optimizer['name'])\n",
    "    p1, = ax.plot([tf.Variable(-5.0)], [tf.Variable(-2.0)], 'kx')\n",
    "    p2, = ax.plot([], [], color='red', alpha=0.6)\n",
    "    p3, = ax.plot([], [], 'r.')\n",
    "    max_iter = optimizer['max_iter']\n",
    "    anim2 = animation.FuncAnimation(fig1, animate_optimizer, init_func=init_animate_optimizer, frames=range(0, max_iter), fargs= (opt, loss_func,), blit=True, interval=50,repeat=True, repeat_delay=20)\n",
    "    filename = optimizer['name']+'.gif'\n",
    "    anim2.save('output/'+out_file_name_prefix+filename, dpi=80, writer='imagemagick', fps=5)\n",
    "    anim2.save('output/ronen/Downloads/'+out_file_name_prefix+filename, dpi=80, writer='imagemagick', fps=5)\n",
    "\n",
    "    init_animate_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eba3d1",
   "metadata": {},
   "source": [
    "# Run optimizations - Contout animation with loss local minima - first execute  box above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878b62ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_offset=0\n",
    "y_offset=0\n",
    "loss_local_min = lambda:  (x +x_offset)**4 - 10 * (x +x_offset)** 2 - 3 * (x +x_offset)+ Y_COEF*(y+y_offset)**2\n",
    "loss_func = loss_local_min\n",
    "out_file_name_prefix = 'loss_local_minima_2d_contour_'\n",
    "loss_with_args = lambda x_cord, y_cord: (x +x_offset)**4 - 10 * (x +x_offset)** 2 - 3 * (x +x_offset)+ Y_COEF*(y+y_offset)**2\n",
    "\n",
    "\n",
    "optimizers_list = []\n",
    "\n",
    "optimizers_list.append({'name': 'sgd', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha), 'max_iter': 100})\n",
    "optimizers_list.append({'name': 'momentum', 'opt': tf.keras.optimizers.SGD(learning_rate=alpha, momentum=0.9), 'max_iter': 100})\n",
    "optimizers_list.append({'name': 'adam', 'opt':  tf.keras.optimizers.Adam(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
    "    name='Adam'), 'max_iter': 200})\n",
    "optimizers_list.append({'name': 'adam', 'opt':  tf.keras.optimizers.Adamax(\n",
    "    learning_rate=alpha, beta_1=0.9, beta_2=0.999, epsilon=1e-07,\n",
    "    name='Adamax'), 'max_iter': 200})\n",
    "\n",
    "\n",
    "                   \n",
    "for optimizer in optimizers_list:\n",
    "    print(optimizer['name'])\n",
    "    opt = optimizer['opt']\n",
    "    plt.clf\n",
    "    fig1, ax = plot_contour(loss_with_args=loss_with_args, title=optimizer['name'])\n",
    "    p1, = ax.plot([tf.Variable(-5.0)], [tf.Variable(-2.0)], 'kx')\n",
    "    p2, = ax.plot([], [], color='red', alpha=0.6)\n",
    "    p3, = ax.plot([], [], 'r.')\n",
    "    max_iter = optimizer['max_iter']\n",
    "    anim2 = animation.FuncAnimation(fig1, animate_optimizer, init_func=init_animate_optimizer, frames=range(0, max_iter), fargs= (opt, loss_func,), blit=True, interval=50,repeat=True, repeat_delay=20)\n",
    "    filename = optimizer['name']+'.gif'\n",
    "    anim2.save('/home/ronen/Downloads/'+out_file_name_prefix+filename, dpi=80, writer='imagemagick', fps=5)\n",
    "    init_animate_optimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899d81c0",
   "metadata": {},
   "source": [
    "# DEmo multi plot animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa5b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Qt5Agg') #use Qt5 as backend, comment this line for default backend\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import animation\n",
    "\n",
    "fig = plt.figure()\n",
    "\n",
    "ax = plt.axes(xlim=(0, 2), ylim=(0, 100))\n",
    "\n",
    "N = 4\n",
    "lines = [plt.plot([], [])[0] for _ in range(N)] #lines to animate\n",
    "\n",
    "rectangles = plt.bar([0.5,1,1.5],[50,40,90],width=0.1) #rectangles to animate\n",
    "\n",
    "patches = lines + list(rectangles) #things to animate\n",
    "\n",
    "def init():\n",
    "    #init lines\n",
    "    for line in lines:\n",
    "        line.set_data([], [])\n",
    "\n",
    "    #init rectangles\n",
    "    for rectangle in rectangles:\n",
    "        rectangle.set_height(0)\n",
    "\n",
    "    return patches #return everything that must be updated\n",
    "\n",
    "def animate(i):\n",
    "    #animate lines\n",
    "    for j,line in enumerate(lines):\n",
    "        line.set_data([0, 2], [10 * j,i])\n",
    "\n",
    "    #animate rectangles\n",
    "    for j,rectangle in enumerate(rectangles):\n",
    "        rectangle.set_height(i/(j+1))\n",
    "\n",
    "    return patches #return everything that must be updated\n",
    "\n",
    "anim = animation.FuncAnimation(fig, animate, init_func=init,\n",
    "                               frames=100, interval=20, blit=True)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72426af5ec3cce56bef682ad834669827d6f5add9d3dc65518dc2e4de0e7fc82"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
